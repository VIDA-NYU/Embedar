--- transformer_BACK.py	2024-07-02 12:02:31.662293970 -0400
+++ transformer.py	2024-07-01 21:01:49.186571396 -0400
@@ -112,7 +112,8 @@
         if not use_flash_attn:
             self.attn = nn.MultiheadAttention(d_model, n_head, dropout=attn_drop)
         else:
-            self.attn = FlashMHA(d_model, n_head, cross_attn=False, bias=True, dropout=attn_drop, use_flash_attn=True)
+#            self.attn = FlashMHA(d_model, n_head, cross_attn=False, bias=True, dropout=attn_drop, use_flash_attn=True)
+            self.attn = FlashMHA(d_model, n_head, cross_attn=False, dropout=attn_drop, use_flash_attn=True)
         self.ls_1 = LayerScale(d_model, ls_init_value) if ls_init_value is not None else nn.Identity()
 
         self.ln_2 = norm_layer(d_model)
